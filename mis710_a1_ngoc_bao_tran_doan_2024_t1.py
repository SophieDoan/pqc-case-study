# -*- coding: utf-8 -*-
"""MIS710_A1_Ngoc_Bao_Tran_Doan_2024_T1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KaGWkXqgqtdWImlN31j-RmCpZXr8SDkg

<a href="https://colab.research.google.com/github/thuc-github/MIS710-T12023/blob/main/A1/MIS710%20A1%20Template%20T1%202023.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# MIS710 Machine Learning in Business - Assignment 1

**Student Name:** Ngoc Bao Tran Doan

**Student ID:** 224779521

<a id = "cell_executivesummary"></a>
##**0. Business understandings, problem, solution and recommendations**

#### NOTE: all the instructions and hints given in this template should be removed from your final submission. (Including this line)

**Business Understandings and Problems**

Play Quest Conquer (PQC) is an online gaming platform in Sydney. Their business model is a subscription model where users pay to subscribe to access, purchase engage with and rate an extensive range of games. PQC has observed significant variability in game ratings across their platforms.

**Objective**

Our objective is to identify and predict the factors that most strongly influence game ratings on platform using data-driven techniques. The project involves the application of machine learning models, including Multi-Linear Regression and Random Forest, employed to analyze and predict game ratings, thereby informing strategic business decisions that can improve user engagement, retention and the company revenue.

**Recommendations**
•	To optimise rater engagement, implement strategies such as gamified rating systems and partnerships with gaming influencers for more user-generated content.
•	There is a need to balance the complexity levels to improve engagement and satisfaction, which can lead to higher average ratings.
•	Utilise data insights and analysis to guide new game development and marketing campaigns, focusing more on highlighting features that drive higher ratings.

## **1. Import libraries**
"""

#load libraries
import pandas as pd #for data manipulation and analysis
import numpy as np #for working with arrays

#import data visualisation libraries
import matplotlib.pyplot as plt
import seaborn as sns

"""##**2. Mount your Google drive**"""

from google.colab import drive
drive.mount('/content/drive')

"""## **3. Exploratory Data Analysis**


1.   Load the dataset
2.   Inspect the data
3.   Initial cleansing


"""

records = pd.read_csv("/PQC_data (1).csv")
print(records)

print('Sample size:', records.shape[0])
print('Number of columns:', records.shape[1])

print("Shape of records: {}".format(records.shape))

records.describe()

records.head()

records.info()

"""###**3.1 Explore the Categorical Variables**


"""

categorical = [col for col in records.columns if records[col].dtype=='O']
print('There are {} numerical variables'.format(len(categorical)))
print(categorical)
records[categorical].head()

"""###**3.2 Explore the Numerical Variables**"""

numerical = [col for col in records.columns if records[col].dtype!='O']
print('There are {} numerical variables'.format(len(numerical)))
print(numerical)
records[numerical].head()

"""###**3.3 Estimate Correlation Coefficients Between Average Ratings and Numerical Variables**"""

import pandas as pd
numerical_records = records.select_dtypes(include=['number'])
(numerical_records.corr().style.background_gradient(cmap='RdBu', vmin=-1, vmax=1))

import pandas as pd
numerical_records = records.select_dtypes(include=['number'])
corr_matrix = numerical_records.corr()
(corr_matrix.style.background_gradient(cmap='RdBu', vmin=-1, vmax=1))
corr_matrix['Average_Rating'].sort_values(ascending=False)

plt.figure(figsize=(16,10))
plt.title('Correlation of Attributes with Average_Rating')
a = sns.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='White')
a.set_xticklabels(a.get_xticklabels(), rotation=90)
a.set_yticklabels(a.get_yticklabels(), rotation=30)
plt.show()

"""###**3.4 Explore Relationships Between Average Ratings and Categorical Variables**"""

# List of categorical features based on metadata
categorical_features = ['Game_Type', 'Age_Category']

# Plot box plots for categorical features
plt.figure(figsize=(20, len(categorical_features) * 5))
for i, feature in enumerate(categorical_features):
    plt.subplot(len(categorical_features), 1, i + 1)
    sns.boxplot(x=records[feature], y=records['Average_Rating'])
    plt.title(f'Relationship between {feature.replace("_", " ")} and Average Rating')
    plt.xlabel(feature.replace('_', ' '))
    plt.ylabel('Average Rating')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

"""###**3.5 General Information and Game Configuration Summary Summary**"""

plt.figure(figsize=(14, 16))

# Histogram for Released_Year
plt.subplot(3, 2, 1)
sns.histplot(data=records[records['Released_Year'] >= 1950], x='Released_Year', bins=30, kde=True)
plt.title("Histogram of Released Year")
plt.xlabel('Released Year')
plt.ylabel('Count')
plt.xticks(rotation=45)

# Histogram for Max_Players
plt.subplot(3, 2, 2)
sns.histplot(data=records, x='Max_Players', bins=20, kde=True)
plt.title("Histogram of Max Players")
plt.xlabel('Max Players')
plt.ylabel('Count')
plt.xlim(0, 20)

# Bar plot for Age_Category
plt.subplot(3, 2, 3)
sns.countplot(data=records, x='Age_Category')
plt.title("Distribution of Age Category")
plt.xlabel('Age Category')
plt.ylabel('Count')
plt.xticks(rotation=45)

# Histogram for Min_Players
plt.subplot(3, 2, 4)
sns.histplot(data=records, x='Min_Players', bins=5, kde=True)
plt.title("Distribution of Min Players")
plt.xlabel('Min Players')
plt.ylabel('Count')
plt.xlim(0, 5)

# Bar plot for Game_Type
plt.subplot(3, 2, 5)
sns.countplot(data=records, x='Game_Type')
plt.title("Distribution of Game Type")
plt.xlabel('Game Type')
plt.ylabel('Count')
plt.xticks(rotation=45)

# Histogram for Average_Playtime
plt.subplot(3, 2, 6)
sns.histplot(data=records, x='Average_Play_Time', bins=20, kde=True)
plt.title("Histogram of Average Play Time")
plt.xlabel('Average Playtime')
plt.ylabel('Count')
plt.xlim(0, 175)

plt.tight_layout()
plt.show()

pd.set_option('display.float_format', lambda x: '%.3f' % x)
data_types =['object', 'float', 'int']
records[['Game_Type', 'Age_Category', 'Min_Players', 'Max_Players','Released_Year']].describe(include=data_types)

"""##**4. Inspect and treat missing data**

###**4.1 Detect missing values**
"""

print(records.isnull().sum().sort_values(ascending=0))

#Fill in missing Game_Name categorical data with mode
records['Game_Name'].fillna(records['Game_Name'].mode()[0], inplace=True)

print(records.isnull().sum().sort_values(ascending=0))

"""We can confirm that there are no missing values in the dataset.

###**4.2 Outliers in discrete variables**
"""

records.describe()

for var in ['Released_Year', 'Min_Players', 'Max_Players', 'Average_Complexity', 'Complexity_Raters', 'Average_Play_Time', 'Owner_Number', 'Trader_Number', 'HighInterest_Number', 'Interest_Number', 'Rater_Number', 'Comment_Number']:
    print(records[var].value_counts() / np.float64(len(records)))
    print()

# Visualize outliers in these variables using boxplots
plt.figure(figsize=(15,10))

plt.subplot(2,3,1)
sns.boxplot(x=records['Released_Year'])
plt.title('Boxplot of Released_Year')

plt.subplot(2,3,2)
sns.boxplot(x=records['Min_Players'])
plt.title('Boxplot of Min_Players')

plt.subplot(2,3,3)
sns.boxplot(x=records['Max_Players'])
plt.title('Boxplot of Max_Players')

plt.subplot(2,3,4)
sns.boxplot(x=records['Average_Complexity'])
plt.title('Boxplot of Average_Complexity')

plt.subplot(2,3,5)
sns.boxplot(x=records['Average_Play_Time'])
plt.title('Boxplot of Average_Play_Time')

plt.tight_layout()
plt.show()

plt.figure(figsize=(15,10))

plt.subplot(2,3,1)
sns.boxplot(x=records['Owner_Number'])
plt.title('Boxplot of Owner_Number')

plt.subplot(2,3,2)
sns.boxplot(x=records['Trader_Number'])
plt.title('Boxplot of Trader_Number')

plt.subplot(2,3,3)
sns.boxplot(x=records['HighInterest_Number'])
plt.title('Boxplot of HighInterest_Number')

plt.subplot(2,3,4)
sns.boxplot(x=records['Interest_Number'])
plt.title('Boxplot of Interest_Number')

plt.subplot(2,3,5)
sns.boxplot(x=records['Rater_Number'])
plt.title('Boxplot of Rater_Number')

plt.tight_layout()
plt.show()

plt.figure(figsize=(6,4))
sns.boxplot(x=records['Average_Rating'])
plt.title('Boxplot of Average_Rating')
plt.show()

plt.figure(figsize=(8, 6))  # Adjust figure size as needed
sns.boxplot(x=records['Average_Play_Time'])
plt.title('Boxplot of Average Play Time')
plt.xlabel('Average Play Time')
plt.show()

"""From the above plot, we can see that the discrete variables show values that are shared by a tiny proportion of variable values in the dataset. For linear regression modeling, this does not cause any problem.

##**5. Build the First Model**
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error

# Perform one-hot encoding on categorical variables
records_encoded = pd.get_dummies(records, columns=['Age_Category', 'Game_Type'], drop_first=True)

# Define the features and target variable
features = [
    'Released_Year', 'Min_Players', 'Max_Players', 'Average_Complexity', 'Complexity_Raters',
    'Average_Play_Time', 'Owner_Number', 'Trader_Number', 'HighInterest_Number',
    'Interest_Number', 'Rater_Number', 'Comment_Number'
] + list(records_encoded.columns[records_encoded.columns.str.startswith('Age_Category_')]) + \
    list(records_encoded.columns[records_encoded.columns.str.startswith('Game_Type_')])

X = records_encoded[features]
y = records_encoded['Average_Rating']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)

# Initialize the Linear Regression model
lm = LinearRegression()

# Fit the model on the training data
lm.fit(X_train, y_train)

# Predict on the test data
y_pred = lm.predict(X_test)

# Evaluate the model
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)
mae = mean_absolute_error(y_test, y_pred)

# Print the model's performance
print(f'R² Score: {r2:.4f}')
print(f'RMSE: {rmse:.4f}')
print(f'MAE:{mae:.4f}')

# Print the coefficients
coefficients = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])
print(coefficients)

"""##**6. Build the Second Model by Reselecting the Features**"""

records_encoded = pd.get_dummies(records, columns=['Age_Category', 'Game_Type'], drop_first=True)
features = ['Min_Players', 'Average_Complexity','Average_Play_Time'] + list(records_encoded.columns[records_encoded.columns.str.startswith('Age_Category_')]) + \
    list(records_encoded.columns[records_encoded.columns.str.startswith('Game_Type_')])
X = records_encoded[features]

y=records['Average_Rating']
y.head()

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2023)

"""##**7. Training the Model**"""

from sklearn.linear_model import LinearRegression

lm = LinearRegression()

lm.fit(X_train,y_train)

coefficients = pd.DataFrame(lm.coef_, X.columns, columns=['Coefficient'])
print(coefficients)

predictions = lm.predict(X_test)

"""##**8. Predicting Test Data**"""

plt.scatter(y_test,predictions)
plt.xlabel('Y Test')
plt.ylabel('Predicted Y')

"""##**9. Evaluating the Model**

"""

print('R^2:', r2_score(y_test, predictions))
print('MAE:', mean_absolute_error(y_test, predictions))
print('MSE:', mean_squared_error(y_test, predictions))

"""##**10. Residuals**

Let's quickly explore the residuals to make sure everything was okay with our data.

**Plot a histogram of the residuals and make sure it looks normally distributed**
"""

print("Shape of y_test:", y_test.shape)
print("Shape of predictions:", predictions.shape)
# Calculate the difference between actual and predicted values
residuals = y_test - predictions[:len(y_test)]
# Plot the distribution of residuals
sns.distplot(residuals, bins=50)
plt.show()

"""##**11. Random Forest Training & Performance**"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import r2_score, mean_squared_error

# Assuming 'records' is your DataFrame containing the data

# Specify the features and target variable
features = ['Game_Type', 'Age_Category', 'Released_Year', 'Min_Players', 'Max_Players',
            'Average_Complexity', 'Complexity_Raters', 'Average_Play_Time', 'Owner_Number',
            'Trader_Number', 'HighInterest_Number', 'Interest_Number', 'Rater_Number',
            'Comment_Number']
target = 'Average_Rating'

# Split the data into training and testing sets
X = records[features]
y = records[target]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a column transformer to handle categorical variables
preprocessor = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(drop='first'), ['Game_Type', 'Age_Category'])
    ],
    remainder='passthrough'  # Leave the rest of the columns unchanged
)

# Create a pipeline that first preprocesses the data and then applies the Random Forest model
model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('regressor', RandomForestRegressor(random_state=42))
])

# Train the model
model.fit(X_train, y_train)

# Predict on the test data
y_pred = model.predict(X_test)

# Evaluate the model
r2 = r2_score(y_test, y_pred)
rmse = mean_squared_error(y_test, y_pred, squared=False)

print(f'R² Score: {r2:.4f}')
print(f'RMSE: {rmse:.4f}')

# Get the feature importances
importances = model.named_steps['regressor'].feature_importances_

# Get the feature names after one-hot encoding
categorical_features = model.named_steps['preprocessor'].transformers_[0][1].get_feature_names_out(['Game_Type', 'Age_Category'])
numerical_features = features[2:]  # The rest of the features are numerical

# Combine the feature names
all_feature_names = list(categorical_features) + numerical_features

# Create a DataFrame for better visualization
feature_importances = pd.DataFrame({'Feature': all_feature_names, 'Importance': importances})

# Sort feature importances by importance
feature_importances = feature_importances.sort_values(by='Importance', ascending=False)

# Display the feature importances
print(feature_importances)

"""##**12. Random Forest Feature Importance Visualisation**"""

plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importances)
plt.title('Random Forest Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.show()